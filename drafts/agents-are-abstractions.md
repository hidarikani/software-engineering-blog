# Agents, can be thought of us another abstraction layer

On computers were conceived it was decided that the best way to store the town program was the binary system. Although those CPS have no issues with that, I think pages of one and zeros isn't intuitive for a human.

The second step was switching to assembly. As far as I can't remember the comments are letter acronyms and dealing with letters easier than ones and zeros.

Then the race began to create more and more high-level programming languages that would help for humans to better conceptualize the code. I personally remember coding in C and C++ and finally C#. At some point there was this idea of visual coding that you could just dragon drop stuff without writing much code, but I guess overtime it stalled because driving and dropping isn't flexible enough to create an actual enterprise application. There is also this sort of that everything should be an object and that would and that's OOP would solve all such engineering problems which I did not. There was also this move to dynamically type languages thinking that worrying about types is cumbersome for human beings and that's it should be solved automatically. As a modern JavaScript developer that it has created a whole class of problems on its own and a new language called TypeScript was born just to fix those problems. Another thing that was deemed to complicated for humans to manage manually was garbage collection. I think that the garbage collection stick on is commonly used right now. I suspect that it's suboptimal and manual tweaking with to make it better but engineers just ignore it for the most part when running on modern high performance computers which hide this effect just by the sheer speed.

Now it's just that we're moving to the age of enterprise systems powered by agents. Let's break the definition of the on the agent first. I would call it a software system that is fundamentally powered by a foundation model. The foundation model is usually a large language model trained using the learning that mimics some neurological processes found in biological beings. The programming in this case is training on enormous publicly available data sets which is basically training on the information available on the Internet. There are also additional rules added after training in the form of text prompts. And then you can also be guardrails enforced by classical programming which are algorithms. Basically algorithms are drawing red lines that cannot be crossed and if this other line is doing something unacceptable of a classical algorithm was just block execution.

 All of this being said it seems that programming as you know it is just moving yet another abstraction layer. It's as if somebody has deemed coding to complicate it for most people and natural language more accessible in that way the way the system should work will be described using natural language. However, it might create a whole set of problems on its own similarly as an introduction of dynamically typed languages. The English language is inherently imprecise, a sentence and can be interpreted in many different ways. People that are implementing agents these days are facing this problems already. I'm treating the introduction of algorithmic  guard drills as evidence of my claim.
