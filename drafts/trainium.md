Have you heard about a new chemical element called Trainium? It comes after Uraniumâ€”just kidding ğŸ˜. Trainium is AWSâ€™s in-house machine learning accelerator. At the recent re:Invent keynote, the AWS CEO made it clear that the company plans to expand its use of custom silicon, including Graviton CPUs, rather than remain indefinitely dependent on NVIDIA.

NVIDIAâ€™s flagship GPUs are likely still ahead in raw performance, but AWS is optimizing for priceâ€“performance at massive scale. By designing its own chips and tightly integrating them with its infrastructure, AWS can offer lower training costs per model and more predictable economics for large, long-running workloadsâ€”even if peak performance is lower.

Itâ€™s also useful to be precise with terminology. NVIDIAâ€™s data-center products are GPUs: originally graphics processors that evolved into highly programmable, general-purpose compute devices via CUDA. Trainium, by contrast, is an ASICâ€”a domain-specific accelerator built specifically for machine learning training. GPUs trade efficiency for flexibility; ASICs trade flexibility for efficiency. That distinction matters more than the old â€œGPUâ€ label when talking about modern data-center compute.